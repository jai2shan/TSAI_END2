{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_Quiz.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "edhClIWVO0km"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jai2shan/TSAI_END2/blob/main/Session%204/RNN_Quiz.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jofyc9OC4Qcf"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahBVnrNc3E0U"
      },
      "source": [
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "plt.style.use('seaborn-white')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crQSAaIz4SkA"
      },
      "source": [
        "# Read and process data. \n",
        "\n",
        "Download the file from this URL: https://drive.google.com/file/d/1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WYQWjTLak6V",
        "outputId": "03c06e7c-b70f-48c7-c611-2832206fb4c8"
      },
      "source": [
        "! gdown https://drive.google.com/uc?id=1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1UWWIi-sz9g0x3LFvkIZjvK1r2ZaCqgGS\n",
            "To: /content/text.txt\n",
            "\r  0% 0.00/10.3k [00:00<?, ?B/s]\r100% 10.3k/10.3k [00:00<00:00, 10.0MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgOGxPDP3Wpp"
      },
      "source": [
        "data = open('text.txt', 'r').read()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeXXMLRb4kXb"
      },
      "source": [
        "Process data and calculate indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5TKeiOp4jtl",
        "outputId": "1a8b14d7-9087-48c3-8f59-eb555a9f2a4d"
      },
      "source": [
        "chars = list(set(data))\n",
        "data_size, X_size = len(data), len(chars)\n",
        "print(\"Corona Virus article has %d characters, %d unique characters\" %(data_size, X_size))\n",
        "char_to_idx = {ch:i for i,ch in enumerate(chars)}\n",
        "idx_to_char = {i:ch for i,ch in enumerate(chars)}"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Corona Virus article has 10223 characters, 75 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4C53MB135LRY"
      },
      "source": [
        "# Constants and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfj21ORa49Ps"
      },
      "source": [
        "Hidden_Layer_size = 10 #size of the hidden layer\n",
        "Time_steps = 10 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OdmJf4Du5uhb"
      },
      "source": [
        "# Activation Functions and Derivatives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seGHei_D5FGk"
      },
      "source": [
        "def sigmoid(x): \n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def dsigmoid(y): \n",
        "    return y * (1 - y)\n",
        "\n",
        "def tanh(x): \n",
        "    return np.tanh(x)\n",
        "\n",
        "def dtanh(y): \n",
        "    return 1 - y * y"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE5sBCokMQPk"
      },
      "source": [
        "# Quiz Question 1\n",
        "\n",
        "What is the value of sigmoid(0) calculated from  your code? (Answer up to 1 decimal point, e.g. 4.2 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldaF0o7mMbS1"
      },
      "source": [
        "# Quiz Question 2\n",
        "\n",
        "What is the value of dsigmoid(sigmoid(0)) calculated from your code?? (Answer up to 2 decimal point, e.g. 4.29 and NOT 4.29999999, no rounding off). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78D9IUC5MhTn"
      },
      "source": [
        "# Quiz Question 3\n",
        "\n",
        "What is the value of tanh(dsigmoid(sigmoid(0))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeCvVH1v6Me-"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Quiz Question 4\n",
        "\n",
        "What is the value of dtanh(tanh(dsigmoid(sigmoid(0)))) calculated from your code?? (Answer up to 5 decimal point, e.g. 4.29999 and NOT 4.29999999, no rounding off)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DECYTOdoMPeW",
        "outputId": "545a297a-8795-4c00-f5cc-520399c54cc3"
      },
      "source": [
        "sigmoid(0)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D1i37ID6MdDo",
        "outputId": "983c1b6b-dc3b-4166-8348-50ff81ea44ae"
      },
      "source": [
        "dsigmoid(sigmoid(0))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwwrPTopMi7M",
        "outputId": "1752a1ce-487a-4c34-b067-6bc6c64a15e4"
      },
      "source": [
        "tanh(dsigmoid(sigmoid(0)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.24491866240370913"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8eRxHzPMoBr",
        "outputId": "9e4e14f1-1b88-46c3-a8fd-ffe81d56ea77"
      },
      "source": [
        "dtanh(tanh(dsigmoid(sigmoid(0))))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.940014848806378"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeSVipDu8iKE"
      },
      "source": [
        "# Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICbWNemE6LGV"
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j83pZNPE8212"
      },
      "source": [
        "We use random weights with normal distribution (0, weight_sd) for  tanh  activation function and (0.5, weight_sd) for  `sigmoid`  activation function.\n",
        "\n",
        "Biases are initialized to zeros."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swHwLXOI9E7V"
      },
      "source": [
        "# LSTM \n",
        "You are making this network, please note f, i, c and o (also \"v\") in the image below:\n",
        "![alt text](http://blog.varunajayasiri.com/ml/lstm.svg)\n",
        "\n",
        "Please note that we are concatenating the old_hidden_vector and new_input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0DBzNY-90s5"
      },
      "source": [
        "# Quiz Question 5\n",
        "\n",
        "In the class definition below, what should be size_a, size_b, and size_c? ONLY use the variables defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFuHhqVq6Wge"
      },
      "source": [
        "size_a = Hidden_Layer_size # write your code here\n",
        "size_b = z_size # write your code here\n",
        "size_c = X_size # write your code here\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzmfGLZt_xVs"
      },
      "source": [
        "Look at these operations which we'll be writing:\n",
        "\n",
        "**Concatenation of h and x:**\n",
        "\n",
        "$z\\:=\\:\\left[h_{t-1},\\:x\\right]$\n",
        "\n",
        "$f_t=\\sigma\\left(W_f\\cdot z\\:+\\:b_f\\:\\right)$\n",
        "\n",
        "$i_i=\\sigma\\left(W_i\\cdot z\\:+\\:b_i\\right)$\n",
        "\n",
        "$\\overline{C_t}=\\tanh\\left(W_C\\cdot z\\:+\\:b_C\\right)$\n",
        "\n",
        "$C_t=f_t\\ast C_{t-1}+i_t\\ast \\overline{C}_t$\n",
        "\n",
        "$o_t=\\sigma\\left(W_o\\cdot z\\:+\\:b_o\\right)$\n",
        "\n",
        "$h_t=o_t\\ast\\tanh\\left(C_t\\right)$\n",
        "\n",
        "**Logits:**\n",
        "\n",
        "$v_t=W_v\\cdot h_t+b_v$\n",
        "\n",
        "**Softmax:**\n",
        "\n",
        "$\\hat{y}=softmax\\left(v_t\\right)$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bUkseNnDott"
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
        "\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZrDhZIjFpdI"
      },
      "source": [
        "You must finish the function above before you can attempt the questions below. \n",
        "\n",
        "# Quiz Question 6\n",
        "\n",
        "What is the output of 'print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))'?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmKU4RP8M53y",
        "outputId": "7ecbd487-7fe0-44b9-c617-b6f886454691"
      },
      "source": [
        "print(len(forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)), parameters)))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XV-YVl_GGiX8"
      },
      "source": [
        "# Quiz Question 7 \n",
        "\n",
        "Assuming you have fixed the forward function, run this command: \n",
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))\n",
        "\n",
        "Now, find these values:\n",
        "\n",
        "\n",
        "1.   print(z.shape)\n",
        "2.   print(np.sum(z))\n",
        "3.   print(np.sum(f))\n",
        "\n",
        "Copy and paste exact values you get in the logs into the quiz.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GvKVWmTDt3H"
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8pdZv9_NBPl",
        "outputId": "6be92e22-edd4-4684-e569-314782292760"
      },
      "source": [
        "print(z.shape)\n",
        "print(np.sum(z))\n",
        "print(np.sum(f))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(85, 1)\n",
            "0.0\n",
            "5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NeSvhkqwILsG"
      },
      "source": [
        "# Backpropagation\n",
        "\n",
        "Here we are defining the backpropagation. It's too complicated, here is the whole code. (Please note that this would work only if your earlier code is perfect)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIa1jUZiGPmF"
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tnc7WpRkIU5S"
      },
      "source": [
        "# Forward and Backward Combined Pass\n",
        "\n",
        "Let's first clear the gradients before each backward pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJWoC3U1ITf8"
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XN93UnjIgmA"
      },
      "source": [
        "Clip gradients to mitigate exploding gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTsublxIfFl"
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T7XUpDTWIl_Y"
      },
      "source": [
        "Calculate and store the values in forward pass. Accumulate gradients in backward pass and clip gradients to avoid exploding gradients.\n",
        "\n",
        "input, target are list of integers, with character indexes.\n",
        "h_prev is the array of initial h at  h−1  (size H x 1)\n",
        "C_prev is the array of initial C at  C−1  (size H x 1)\n",
        "Returns loss, final  hT  and  CT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQNxjTuZIia_"
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcy5u_vRItkV"
      },
      "source": [
        "# Sample the next character"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8SrtJiwIsSm"
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiWFaWLNIx_L"
      },
      "source": [
        "# Training (Adagrad)\n",
        "\n",
        "Update the graph and display a sample output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENQYU-7AIw0t"
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACXcASJuI73a"
      },
      "source": [
        "# Update Parameters\n",
        "\n",
        "\\begin{align}\n",
        "\\theta_i &= \\theta_i - \\eta\\frac{d\\theta_i}{\\sum dw_{\\tau}^2} \\\\\n",
        "d\\theta_i &= \\frac{\\partial L}{\\partial \\theta_i}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bR08TvcjI4Pf"
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "La9vyJ6RJLFK"
      },
      "source": [
        "To delay the keyboard interrupt to prevent the training from stopping in the middle of an iteration\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVDHbMb7JNGT"
      },
      "source": [
        "# Exponential average of loss\n",
        "# Initialize to a error of a random model\n",
        "smooth_loss = -np.log(1.0 / X_size) * Time_steps\n",
        "\n",
        "iteration, pointer = 0, 0\n",
        "\n",
        "# For the graph\n",
        "plot_iter = np.zeros((0))\n",
        "plot_loss = np.zeros((0))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF6vS0VWJqsS"
      },
      "source": [
        "# Training Loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "edhClIWVO0km"
      },
      "source": [
        "## All the previous cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8owA9Ba0O_Z7"
      },
      "source": [
        "Hidden_Layer_size = 100 #size of the hidden layer\n",
        "Time_steps = 40 # Number of time steps (length of the sequence) used for training\n",
        "learning_rate = 1e-1 # Learning Rate\n",
        "weight_sd = 0.1 #Standard deviation of weights for initialization\n",
        "z_size = Hidden_Layer_size + X_size #Size of concatenation(H, X) vector"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdGBImuJO_aE"
      },
      "source": [
        "class Param:\n",
        "    def __init__(self, name, value):\n",
        "      self.name = name\n",
        "      self.v = value # parameter value\n",
        "      self.d = np.zeros_like(value) # derivative\n",
        "      self.m = np.zeros_like(value) # momentum for Adagrad"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS3jVfjjO_aF"
      },
      "source": [
        "size_a = Hidden_Layer_size # write your code here\n",
        "size_b = z_size # write your code here\n",
        "size_c = X_size # write your code here\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.W_f = Param('W_f', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_f = Param('b_f', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_i = Param('W_i', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_i = Param('b_i', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_C = Param('W_C', np.random.randn(size_a, size_b) * weight_sd)\n",
        "        self.b_C = Param('b_C', np.zeros((size_a, 1)))\n",
        "\n",
        "        self.W_o = Param('W_o', np.random.randn(size_a, size_b) * weight_sd + 0.5)\n",
        "        self.b_o = Param('b_o', np.zeros((size_a, 1)))\n",
        "\n",
        "        #For final layer to predict the next character\n",
        "        self.W_v = Param('W_v', np.random.randn(X_size, size_a) * weight_sd)\n",
        "        self.b_v = Param('b_v', np.zeros((size_c, 1)))\n",
        "        \n",
        "    def all(self):\n",
        "        return [self.W_f, self.W_i, self.W_C, self.W_o, self.W_v,\n",
        "               self.b_f, self.b_i, self.b_C, self.b_o, self.b_v]\n",
        "        \n",
        "parameters = Parameters()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cl0p3Hi0O_aG"
      },
      "source": [
        "def forward(x, h_prev, C_prev, p = parameters):\n",
        "    assert x.shape == (X_size, 1)\n",
        "    assert h_prev.shape == (Hidden_Layer_size, 1)\n",
        "    assert C_prev.shape == (Hidden_Layer_size, 1)\n",
        "    \n",
        "    z = np.row_stack((h_prev, x))\n",
        "    f = sigmoid(np.dot(p.W_f.v, z) + p.b_f.v)\n",
        "    i = sigmoid(np.dot(p.W_i.v, z) + p.b_i.v)\n",
        "    C_bar = tanh(np.dot(p.W_C.v, z) + p.b_C.v)\n",
        "\n",
        "    C = f * C_prev + i * C_bar\n",
        "    o = sigmoid(np.dot(p.W_o.v, z) + p.b_o.v)\n",
        "    h = o * tanh(C)\n",
        "\n",
        "    v = np.dot(p.W_v.v, h) + p.b_v.v\n",
        "    y = np.exp(v) / np.sum(np.exp(v)) #softmax\n",
        "\n",
        "    return z, f, i, C_bar, C, o, h, v, y"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TAqCVuAOO_aH"
      },
      "source": [
        "z, f, i, C_bar, C, o, h, v, y = forward(np.zeros((X_size, 1)), np.zeros((Hidden_Layer_size, 1)), np.zeros((Hidden_Layer_size, 1)))"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zysgmuXaO_aH"
      },
      "source": [
        "def backward(target, dh_next, dC_next, C_prev,\n",
        "             z, f, i, C_bar, C, o, h, v, y,\n",
        "             p = parameters):\n",
        "    \n",
        "    assert z.shape == (X_size + Hidden_Layer_size, 1)\n",
        "    assert v.shape == (X_size, 1)\n",
        "    assert y.shape == (X_size, 1)\n",
        "    \n",
        "    for param in [dh_next, dC_next, C_prev, f, i, C_bar, C, o, h]:\n",
        "        assert param.shape == (Hidden_Layer_size, 1)\n",
        "        \n",
        "    dv = np.copy(y)\n",
        "    dv[target] -= 1\n",
        "\n",
        "    p.W_v.d += np.dot(dv, h.T)\n",
        "    p.b_v.d += dv\n",
        "\n",
        "    dh = np.dot(p.W_v.v.T, dv)        \n",
        "    dh += dh_next\n",
        "    do = dh * tanh(C)\n",
        "    do = dsigmoid(o) * do\n",
        "    p.W_o.d += np.dot(do, z.T)\n",
        "    p.b_o.d += do\n",
        "\n",
        "    dC = np.copy(dC_next)\n",
        "    dC += dh * o * dtanh(tanh(C))\n",
        "    dC_bar = dC * i\n",
        "    dC_bar = dtanh(C_bar) * dC_bar\n",
        "    p.W_C.d += np.dot(dC_bar, z.T)\n",
        "    p.b_C.d += dC_bar\n",
        "\n",
        "    di = dC * C_bar\n",
        "    di = dsigmoid(i) * di\n",
        "    p.W_i.d += np.dot(di, z.T)\n",
        "    p.b_i.d += di\n",
        "\n",
        "    df = dC * C_prev\n",
        "    df = dsigmoid(f) * df\n",
        "    p.W_f.d += np.dot(df, z.T)\n",
        "    p.b_f.d += df\n",
        "\n",
        "    dz = (np.dot(p.W_f.v.T, df)\n",
        "         + np.dot(p.W_i.v.T, di)\n",
        "         + np.dot(p.W_C.v.T, dC_bar)\n",
        "         + np.dot(p.W_o.v.T, do))\n",
        "    dh_prev = dz[:Hidden_Layer_size, :]\n",
        "    dC_prev = f * dC\n",
        "    \n",
        "    return dh_prev, dC_prev"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eeBM6dyVO_aI"
      },
      "source": [
        "def clear_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.d.fill(0)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRE1ycA0O_aI"
      },
      "source": [
        "def clip_gradients(params = parameters):\n",
        "    for p in params.all():\n",
        "        np.clip(p.d, -1, 1, out=p.d)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsHpPe3gO_aJ"
      },
      "source": [
        "def forward_backward(inputs, targets, h_prev, C_prev):\n",
        "    global paramters\n",
        "    \n",
        "    # To store the values for each time step\n",
        "    x_s, z_s, f_s, i_s,  = {}, {}, {}, {}\n",
        "    C_bar_s, C_s, o_s, h_s = {}, {}, {}, {}\n",
        "    v_s, y_s =  {}, {}\n",
        "    \n",
        "    # Values at t - 1\n",
        "    h_s[-1] = np.copy(h_prev)\n",
        "    C_s[-1] = np.copy(C_prev)\n",
        "    \n",
        "    loss = 0\n",
        "    # Loop through time steps\n",
        "    assert len(inputs) == Time_steps\n",
        "    for t in range(len(inputs)):\n",
        "        x_s[t] = np.zeros((X_size, 1))\n",
        "        x_s[t][inputs[t]] = 1 # Input character\n",
        "        \n",
        "        (z_s[t], f_s[t], i_s[t],\n",
        "        C_bar_s[t], C_s[t], o_s[t], h_s[t],\n",
        "        v_s[t], y_s[t]) = \\\n",
        "            forward(x_s[t], h_s[t - 1], C_s[t - 1]) # Forward pass\n",
        "            \n",
        "        loss += -np.log(y_s[t][targets[t], 0]) # Loss for at t\n",
        "        \n",
        "    clear_gradients()\n",
        "\n",
        "    dh_next = np.zeros_like(h_s[0]) #dh from the next character\n",
        "    dC_next = np.zeros_like(C_s[0]) #dh from the next character\n",
        "\n",
        "    for t in reversed(range(len(inputs))):\n",
        "        # Backward pass\n",
        "        dh_next, dC_next = \\\n",
        "            backward(target = targets[t], dh_next = dh_next,\n",
        "                     dC_next = dC_next, C_prev = C_s[t-1],\n",
        "                     z = z_s[t], f = f_s[t], i = i_s[t], C_bar = C_bar_s[t],\n",
        "                     C = C_s[t], o = o_s[t], h = h_s[t], v = v_s[t],\n",
        "                     y = y_s[t])\n",
        "\n",
        "    clip_gradients()\n",
        "        \n",
        "    return loss, h_s[len(inputs) - 1], C_s[len(inputs) - 1]"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L368yJm7O_aK"
      },
      "source": [
        "def sample(h_prev, C_prev, first_char_idx, sentence_length):\n",
        "    x = np.zeros((X_size, 1))\n",
        "    x[first_char_idx] = 1\n",
        "\n",
        "    h = h_prev\n",
        "    C = C_prev\n",
        "\n",
        "    indexes = []\n",
        "    \n",
        "    for t in range(sentence_length):\n",
        "        _, _, _, _, C, _, h, _, p = forward(x, h, C)\n",
        "        idx = np.random.choice(range(X_size), p=p.ravel())\n",
        "        x = np.zeros((X_size, 1))\n",
        "        x[idx] = 1\n",
        "        indexes.append(idx)\n",
        "\n",
        "    return indexes"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUPQpEimO_aK"
      },
      "source": [
        "def update_status(inputs, h_prev, C_prev):\n",
        "    #initialized later\n",
        "    global plot_iter, plot_loss\n",
        "    global smooth_loss\n",
        "    \n",
        "    # Get predictions for 200 letters with current model\n",
        "\n",
        "    sample_idx = sample(h_prev, C_prev, inputs[0], 200)\n",
        "    txt = ''.join(idx_to_char[idx] for idx in sample_idx)\n",
        "\n",
        "    # Clear and plot\n",
        "    plt.plot(plot_iter, plot_loss)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "\n",
        "    #Print prediction and loss\n",
        "    print(\"----\\n %s \\n----\" % (txt, ))\n",
        "    print(\"iter %d, loss %f\" % (iteration, smooth_loss))"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ho1YKN0O_aL"
      },
      "source": [
        "def update_paramters(params = parameters):\n",
        "    for p in params.all():\n",
        "        p.m += p.d * p.d # Calculate sum of gradients\n",
        "        #print(learning_rate * dparam)\n",
        "        p.v += -(learning_rate * p.d / np.sqrt(p.m + 1e-8))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVHGY4wUOzHr"
      },
      "source": [
        ""
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PH7ikVbNO3Gg"
      },
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "OQyNSL0iJOxH",
        "outputId": "cc920ad7-2bd7-4eb9-a9e7-1baf25d291b0"
      },
      "source": [
        "iter = 50_000\n",
        "while iter > 0:\n",
        "  # Reset\n",
        "  if pointer + Time_steps >= len(data) or iteration == 0:\n",
        "      g_h_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      g_C_prev = np.zeros((Hidden_Layer_size, 1))\n",
        "      pointer = 0\n",
        "\n",
        "\n",
        "  inputs = ([char_to_idx[ch] \n",
        "              for ch in data[pointer: pointer + Time_steps]])\n",
        "  targets = ([char_to_idx[ch] \n",
        "              for ch in data[pointer + 1: pointer + Time_steps + 1]])\n",
        "\n",
        "  loss, g_h_prev, g_C_prev = \\\n",
        "      forward_backward(inputs, targets, g_h_prev, g_C_prev)\n",
        "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
        "\n",
        "  # Print every hundred steps\n",
        "  if iteration % 100 == 0:\n",
        "      update_status(inputs, g_h_prev, g_C_prev)\n",
        "\n",
        "  update_paramters()\n",
        "\n",
        "  plot_iter = np.append(plot_iter, [iteration])\n",
        "  plot_loss = np.append(plot_loss, [loss])\n",
        "\n",
        "  pointer += Time_steps\n",
        "  iteration += 1\n",
        "  iter = iter -1"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "----\n",
            " tes, conters by a ace that cough, or people coughing and South 38 al ferson, including not danding whening serioned from Wuhan and endicali\n",
            "\n",
            "Apal ot belity mopucentervertile knters. In A hoid ng.\n",
            "Ther \n",
            "----\n",
            "iter 49900, loss 5.096868\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AKpa1BGOItQ"
      },
      "source": [
        "# Quiz Question 8\n",
        "\n",
        "Run the above code for 50000 iterations making sure that you have 100 hidden layers and time_steps is 40. What is the loss value you're seeing?"
      ]
    }
  ]
}